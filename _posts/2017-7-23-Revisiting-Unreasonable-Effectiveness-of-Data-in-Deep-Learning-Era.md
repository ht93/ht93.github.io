Paper: [arxiv](https://arxiv.org/abs/1707.02968)  
Blog post: [blog link](https://research.googleblog.com/2017/07/revisiting-unreasonable-effectiveness.html?utm_content=bufferd6616&utm_medium=social&utm_source=twitter.com&utm_campaign=buffer&m=1)  

**Key idea**: 
>We believe that, although challenging, obtaining large scale task-specific data should be the focus of future study.

**Some points**:  
* Better Representation Learning Helps.
* Performance increases linearly with orders of magnitude of training data. 
<img src="https://3.bp.blogspot.com/-szaDUQXe_ak/WWUJ85ysh5I/AAAAAAAAB3o/joW-ItRpiCU6o_FyB-CMpCQ1XU4QFaI3QCEwYBhgL/s1600/image2.png" height="300" />

* Capacity is Crucial. (network capacity need to be large to learn more data)
* New state of the art results. (a single model (without any bells and whistles) can now achieve 37.4 AP as compared to 34.3 AP on the COCO detection benchmark.)
